---
title: Why Are "Agents"?
subtitle: Agents Are Streets Ahead.
created: 2025-09-19
edited: 2025-09-23
tags: ['AI', 'Hot Takes']
---

<h3>What Even?</h3>

<p>A little while ago, I had a discussion with a colleague about AI and what constituted agency. It seems like the new hot thing is for companies to claim their AI is now an agent and accompany it with all the hype and marketing. However, from my perspective, it didn't seem like they were really doing anything new. If I asked the earliest generative LLM models how to create a python file in bash, they could all tell me to "touch grass.py". If I asked it how I ought to organize files for a project, it would give me a logical breakdown of how many files to make and what functions they ought to contain to achieve my goal. It seems like the only difference between what was offered then and what's offered now is that the LLM now has access to code execution and is connected to a dev environment. This means it can just go ahead and do things it already could know how to do. The ability to create files and populate it with code hasn't changed. The only thing that's changed is the architecture the model uses.</p>

<p>Does that make it an "agent?" I'm increasingly thinking that this question isn't very useful. This is for several reasons. First, it seems like every company has a different definition for "agent", and they range from intuitive to incoherent. Secondly, the question is somewhat philosophical. When I answer an email am I exercising agency? Some emails require thought and decision making, and others do not. Do some emails require agency while others don't, or is the entire task of answering an email one that requires agency? If I used to copy my emails into an LLM and copy the output into my reply, but now I give the LLM access to my email and it sends replies automatically, is it now "agentic" since it is now empowered to generate a "send" token in addition to all the other millions of tokens in its vocabulary, and the result is substantively the same as if I were doing copy and paste? </p>

<p>A better way to think about it may be to evaluate agency as a quality instead of a quantity. Instead of something being "agentic" or not, I want to instead ask questions about how agentic something, and in what ways. An LLM could always write an email, and if I'm integrating it into a sort of auto-response, I guess that's a little bit more agentic than it otherwise would be. Instead of deciding to use an LLM to respond to every incoming email, I decide once that I want the LLM to respond to all emails, and it will do it. Instead of telling a coding AI that I want to first generate an outline for a project, then layout a development plan, then write the code and tests, I can tell it that I want it to fill out a project responsive to my prompt with the reasonable expectation that it will follow all these steps. Instead of stopping at every step to re-engage the LLM, these steps are streamlined. All these could be seen as agentic, even if it's more of a different application of an existing technology than an entirely innovative step forward itself. </p>

<p>Personally, I feel like my knee-jerk response to claims of agency is a sort of True Scotsman fallacy. I want to walk the bar for agency back in response to a variety of reasons. To be fair, I think this is pretty justified. It often feels like the industry tries to attach anthropomorphic qualities as labels and conclusions to the product they're selling. Qualitatively evaluating agency is useful for both seeing past this marketing and developing a better understanding of these products. </p>

<p>I have a cronjob that hashes the pdf of the list of cases granted on the SCOTUS's website twice a day. If it detects a change, it sends out an email alert to me and a few other researchers. This has no AI in it whatsoever. In fact, I think it's fewer than 100 lines of code in total and probably could be shorter. Is it agentic? Functionally, it seems as agentic as the hypothetical AI email responder, so I guess? If instead of just responding "The list has been updated. Here is a link to the new list", I called a local Llama model to generate a synopsis of the change to populate the body of the email, would that mean that my lowly email alert system is now an AI agent? I mean, maybe? But this change wouldn't actually change all that much in terms of the usefulness of the alert system.</p>

<h3>Perspective-Labeling Expectations</h3>

<p>I think the hype around "agents" is also fueled by the fact that so many use cases for these "agents" is for programming or programming adjacent. The substance that constitutes programming is primarily writing. Sure, many tasks compose programming- refactoring, cleaning, hunting bugs, etc., but I lump all of these into writing tasks. Other tasks that programmers have to do, e.g. reading documentation, are instrumentalized as a necessary element of writing. And writing is what LLMs fundamentally do. Many LLMs can also now separate tasks into a planning stage and a drafting stage, or will initialize a draft then change it later on. The process looks a lot more like a human workflow needed to arrive at the same end product. Should we then be surprised that the way "agents" are built, marketed, and sold is for implementation into tasks that are fundamentally writing?</p>

<p>Stated differently: since "agents" are made by programmers, the "doing" of programming is writing, and LLMs can write, we ought not to be surprised that programmers claim that the "agents" can "do".</p>

<p>This is where thinking qualitatively can help. If an AI coding agent is what it looks like to be agentic at software programming, what does it look like to be agentic in plumbing? healthcare? custodial services? Being a little snarky, what does it mean for these "agents" to be able to "touch grass.py" for those other areas of human labor?</p>

<h3>Better Questions for Better Answers</h3>

<p>In fact, I believe that it helps to think qualitatively about a lot of characteristics that AI companies advertise in your face. You say the AI is creative, but in what ways and to what extent? You say the AI is intelligent, but what does its intelligence look like? You say the AI can reason, but what sort of reasoning does it do, and in what circumstances does it do it?</p>

<p>There is almost a little ego death with letting go of what you think you know about communication around these products. It is saying that I accept your label of "agentic" or "reasoning" or whatever, but I also accept that I don't know what you mean by those words, so it's your job to communicate it. I will allow my pre-existing knowledge and perspectives to neither work for you nor against you in that task.</p>

<p>But this doesn't mean that the nuance you afford technology should be the same as the nuance you afford humans. A nuanced approach to a painting is a very different from a nuanced approach to an AI model, but in neither case is it all that useful to just label it "creative" or "not creative." A label is cheap and easily slapped onto a product, justified thinly by the obfuscatory fog of nontransparent terms of trade. If you want to have better communication and understanding around these tools, don't be placated with simple nametags on complex ideas.</p>